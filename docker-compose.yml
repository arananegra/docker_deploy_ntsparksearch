version: '2'

services:
  ntsparksearch:
    restart: always
    build:
      context: ./NtSparkSearch
    image: nt_spark_search_image
    hostname: master
    environment:
      MASTER: spark://master:7077
      SPARK_CONF_DIR: /usr/spark-2.1.0/conf
      SPARK_PUBLIC_DNS: localhost
    extra_hosts:
      - "database:172.16.239.11"
      - "redis-server:172.16.239.12"
    networks:
      ci_co_net:
        ipv4_address: 172.16.239.10
    container_name: ntsparksearch_docker_master
    expose:
      - 7001
      - 7002
      - 7003
      - 7004
      - 7005
      - 7006
      - 7077
      - 6066
    ports:
      - 4040:4040
      - 6066:6066
      - 7077:7077
      - 8080:8080
      - 5000:5000
    links:
      - mongodb_ntsparksearch:ntsparksearch_docker_master
      - redis:ntsparksearch_docker_master
      - worker-rq:ntsparksearch_docker_master
      - ntsparksearch_frontend:ntsparksearch_docker_master
    volumes:
      - ./conf/master:/usr/spark-2.1.0/conf/
      - ./data:/tmp/data
      - ./volumes/input_files:/usr/spark-2.1.0/input
      - ./volumes/output_files:/usr/spark-2.1.0/output
    command: /usr/spark-2.1.0/launch_spark_and_service.sh

  worker:
    image: nt_spark_search_image
    container_name: ntsparksearch_docker_worker
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077
    extra_hosts:
      - "master:172.16.239.10"
    hostname: worker
    networks:
      ci_co_net:
        ipv4_address: 172.16.239.14
    environment:
      SPARK_CONF_DIR: /usr/spark-2.1.0/conf
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
      SPARK_PUBLIC_DNS: localhost
    links:
      - ntsparksearch
    expose:
      - 7012
      - 7013
      - 7014
      - 7015
      - 7016
      - 8881
    ports:
      - 8081:8081
    volumes:
      - ./conf/worker:/usr/spark-2.1.0/conf
      - ./data:/tmp/data
  
  mongodb_ntsparksearch:
    image: mongo:3.4.6
    container_name: mongodb_ntsparksearch
    networks:
      ci_co_net:
        ipv4_address: 172.16.239.11
    mem_limit: 256m
    ports:
      - 8082:27017
    volumes:
      - ./volumes/mongodb_recovery:/data/db
    restart: always

  worker-rq:
    image: nt_spark_search_image
    networks:
      ci_co_net:
        ipv4_address: 172.16.239.13
    extra_hosts:
      - "database:172.16.239.11"
      - "redis-server:172.16.239.12"
    container_name: ntsparksearch_docker_worker-rq
    command: python3.4 /usr/spark-2.1.0/ntsparksearch_backend/ntsparksearch/RestApi/worker.py 
    links:
      - redis:ntsparksearch_docker_worker-rq
    depends_on:
    - redis

  redis:
    image: redis:3.2.8
    command: redis-server
    networks:
      ci_co_net:
        ipv4_address: 172.16.239.12
    ports:
      - 6379:6379
    container_name: redis_ntsparksearch
    restart: always

  ntsparksearch_frontend:
    image: arananegra/ntsparksearch_frontend
    build:
      context: ./frontend_server
    restart: always
    networks:
      ci_co_net:
        ipv4_address: 172.16.239.15
    ports:
      - 1313:8080
    container_name: ntsparksearch_frontend_docker

networks:
  ci_co_net:
    driver: bridge
    ipam:
      driver: default
      config:
      - subnet: 172.16.239.0/24
        gateway: 172.16.239.2

    
